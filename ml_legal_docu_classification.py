# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XIU1gnw796C323YtKPZcjp40Ob--0lJs
"""

# Core libraries for data manipulation and analysis
import numpy as np
import pandas as pd

# Libraries for machine learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Libraries for visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Set a style for all plots
sns.set_style("whitegrid")

def plot_decision_boundaries(X, y, model, title):
    """
    Visualizes the decision boundaries of a trained classifier.
    (Corrected version)
    """
    # Create a meshgrid to plot the decision boundary
    h = .02  # step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # Predict the class for each point in the meshgrid
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot the decision boundary and the data points
    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

    # Plot the training points
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)

    # Get unique labels and ensure they are a list for the legend function
    unique_labels = np.unique(y)
    if len(unique_labels) == 2:
        legend_labels = ['Class 0', 'Class 1']
    else:
        legend_labels = list(unique_labels.astype(str)) # Convert numpy array to a list

    plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels)
    plt.show()

"""## Decision Boundary Visualization Function"""

from sklearn.datasets import make_moons

# Generate non-linearly separable data
X_moons, y_moons = make_moons(n_samples=500, noise=0.2, random_state=42)

# Split the data into training and testing sets
X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(
    X_moons, y_moons, test_size=0.3, random_state=42
)

# Scale the features
# Feature scaling is crucial for SVMs to perform optimally.
scaler_moons = StandardScaler()
X_train_moons_scaled = scaler_moons.fit_transform(X_train_moons)
X_test_moons_scaled = scaler_moons.transform(X_test_moons)

"""**bold text**## Data Generation, Splitting, and Scaling"""

# Visualize the Moons dataset
plt.figure(figsize=(8, 6))
plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=plt.cm.coolwarm, edgecolors='k')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Moons Dataset')
plt.show()

""" 'Moons' dataset, which is a made-up dataset often used to show how classification stuff works with data that isn't just a straight line. It looks like two half-circles that fit together, kind of like two moons. When you see this data, it helps you get why simple line-based models might have a hard time sorting it out."""

kernels = ['linear', 'rbf', 'poly']
models_moons = {}
SRN = "PES2UG24CS824" # Your SRN

for kernel in kernels:
    # TODO: Initialize SVM with the current kernel
    # Use random_state=42 for reproducibility
    svm_model = SVC(kernel=kernel, random_state=42)

    # TODO: Train the model
    svm_model.fit(X_train_moons_scaled, y_train_moons)

    # Store the trained model
    models_moons[kernel] = svm_model

    # TODO: Make predictions
    y_pred_moons = svm_model.predict(X_test_moons_scaled)

    # TODO: Replace with your SRN
    print(f"SVM with {kernel.upper()} Kernel {SRN}")
    print(classification_report(y_test_moons, y_pred_moons))
    print("-" * 40 + "\n")

"""## Training and Evaluating SVM Models with  Kernels

Here we train SVM models using different kernels: linear, RBF, and polynomial. We then evaluate their performance on the test set. The results show how each kernel handles the non-linearly separable 'Moons' dataset, highlighting the effectiveness of non-linear kernels like RBF and polynomial.
"""

SRN = "PES2UG24CS824" # Your SRN
#TODO: Replace with your SRN '''''
for kernel, model in models_moons.items():
    plot_decision_boundaries(
        X_train_moons_scaled,
        y_train_moons,
        model,
        title=f'Moons Dataset - SVM with {kernel.upper()} Kernel {SRN}'
    )

"""## Visualizing Decision Boundaries

This cell visualizes the decision boundaries for the SVM models trained with different kernels on the 'Moons' dataset. The plots show how each kernel separates the classes.

- **Linear Kernel:** Creates a straight line boundary, struggling with the 'Moons' dataset.
- **RBF Kernel:** Creates a curved boundary, effectively separating the classes and achieving the highest accuracy.
- **Polynomial Kernel:** Also creates a non-linear boundary, performing better than linear but slightly less than RBF on this data.

These visualizations help understand why non-linear kernels are better for this type of data.
"""

# Load the dataset from a URL
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'
banknote_df = pd.read_csv(url, header=None, names=['variance', 'skewness', 'curtosis', 'entropy', 'class'])

# Select features and target (using all features)
X_banknote = banknote_df[['variance', 'skewness', 'curtosis', 'entropy']].values
y_banknote = banknote_df['class'].values

# Split data
X_train_banknote, X_test_banknote, y_train_banknote, y_test_banknote = train_test_split(
    X_banknote, y_banknote, test_size=0.3, random_state=42, stratify=y_banknote
)

# Scale features
scaler_banknote = StandardScaler()
X_train_banknote_scaled = scaler_banknote.fit_transform(X_train_banknote)
X_test_banknote_scaled = scaler_banknote.transform(X_test_banknote)

"""## Banknote Authentication Data Loading, Splitting, and Scaling

Okay, so this part is about getting the data ready to train our models to tell if a banknote is real or fake.

Here's what we do step-by-step:

1.  **Get the data:** We download the banknote data from a website. It's like getting a big table of numbers.
2.  **Name the columns:** We give names to the columns in the table so we know what the numbers mean (like how much the picture varies or how skewed it is).
3.  **Pick what we need:** We choose the columns we want to use as features (the things that describe the banknotes) and the column that tells us if it's real or fake (that's our target).
4.  **Split the data:** We divide the data into two piles: one pile for the model to learn from (training data) and another pile to test how well it learned (testing data). We make sure both piles have a good mix of real and fake banknotes.
5.  **Scale the numbers:** We adjust the numbers in the features so they are all in a similar range. This helps our SVM models work better.
"""

# Visualize the Banknote Authentication dataset (Note: Visualization is limited to 2 features)
# We are now using all features for training, but the scatter plot can only show two at a time.
# To visualize the impact of all features, we would typically use dimensionality reduction techniques or pairwise plots.
plt.figure(figsize=(8, 6))
plt.scatter(X_banknote[:, 0], X_banknote[:, 1], c=y_banknote, cmap=plt.cm.coolwarm, edgecolors='k')
plt.xlabel('Variance')
plt.ylabel('Skewness')
plt.title('Banknote Authentication Dataset (Showing Variance vs Skewness)')
plt.show()

kernels = ['linear', 'rbf', 'poly']
models_banknote = {}
SRN = "PES2UG24CS824" # Your SRN

for kernel in kernels:
    # TODO: Initialize and train the SVM
    # Set random_state=42 for reproducibility.
    svm_model = SVC(kernel=kernel, random_state=42)
    svm_model.fit(X_train_banknote_scaled, y_train_banknote)

    # Store the model
    models_banknote[kernel] = svm_model

    # TODO: Make predictions
    y_pred_banknote = svm_model.predict(X_test_banknote_scaled)

    # TODO: Replace with your SRN
    print(f"SVM with {kernel.upper()} Kernel {SRN}")
    print(classification_report(y_test_banknote, y_pred_banknote, target_names=['Forged', 'Genuine']))
    print("-" * 40 + "\n")

SRN = "PES2UG24CS824" # Your SRN
#TODO: Replace with your SRN
# The plot_decision_boundaries function is designed for 2 features.
# The banknote dataset now uses 4 features, so we cannot directly visualize the decision boundaries in 2D.
print(f"Decision boundary visualization with {len(X_train_banknote_scaled[0])} features is not directly supported by the current plotting function.")
print("To visualize in 2D, dimensionality reduction techniques would be needed.")

# for kernel, model in models_banknote.items():
#     plot_decision_boundaries(
#         X_train_banknote_scaled,
#         y_train_banknote,
#         model,
#         title=f'Banknote Dataset - SVM with {kernel.upper()} Kernel {SRN}'
#     )

"""## Sample Banknote Predictions
 (the one with the RBF kernel) is doing on a few examples from the banknote test data. It takes the first 10 test data points, asks the model what it thinks they are (real or fake), and then shows us the model's guesses next to the actual answers. It's a quick way to see if the model is predicting correctly on some samples.
"""

# Make predictions on the banknote test set using the RBF kernel model
# The RBF kernel performed well based on the classification report in cell 10.
rbf_model_banknote = models_banknote['rbf']
y_pred_sample_banknote = rbf_model_banknote.predict(X_test_banknote_scaled[:10]) # Predict on the first 10 samples

print("Sample Predictions (RBF Kernel) for Banknote Data:")
print("Predicted:", y_pred_sample_banknote)
print("Actual:   ", y_test_banknote[:10])

# You can also display the first few actual data points and their true labels
print("\nFirst 10 Scaled Banknote Test Data Points and Actual Labels:")
for i in range(10):
    print(f"Data Point {i+1}: {X_test_banknote_scaled[i]} - Actual Label: {y_test_banknote[i]}")

from sklearn.datasets import make_blobs

# Generate linearly separable data with some noise
X_linear, y_linear = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.60)

# Add some outliers
outliers_X = np.array([[0.5, 2.5], [1.5, 0.5]])
outliers_y = np.array([1, 0])
X_linear = np.concatenate([X_linear, outliers_X])
y_linear = np.concatenate([y_linear, outliers_y])


# Split and scale the data
X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(
    X_linear, y_linear, test_size=0.3, random_state=42
)
scaler_linear = StandardScaler()
X_train_linear_scaled = scaler_linear.fit_transform(X_train_linear)
X_test_linear_scaled = scaler_linear.transform(X_test_linear)

SRN = "PES2UG24CS824" # Your SRN

# Soft Margin SVM (small C)
# TODO: Create a linear SVM model with a small C value (e.g., 0.1) for a soft margin.
#          - Set the kernel to 'linear'.
#          - Set C to 0.1.
#          - Set random_state to 42 for consistent results.
svm_soft = SVC(kernel='linear', C=0.1, random_state=42)

# TODO: Fit the soft margin model to the training data (X_train_linear_scaled, y_train_linear).
svm_soft.fit(X_train_linear_scaled, y_train_linear)

# TODO: Replace with your SRN
plot_decision_boundaries(X_train_linear_scaled, y_train_linear, svm_soft, title=f'Soft Margin SVM (C=0.1) {SRN}')

# Hard Margin SVM (large C)
# TODO: Create a linear SVM model with a large C value (e.g., 100) for a hard margin.
#          - Set the kernel to 'linear'.
#          - Set C to 100.
#          - Set random_state to 42.
svm_hard = SVC(kernel='linear', C=100, random_state=42)

# TODO: Fit the hard margin model to the training data
svm_hard.fit(X_train_linear_scaled, y_train_linear)

# TODO: Replace with your SRN
plot_decision_boundaries(X_train_linear_scaled, y_train_linear, svm_hard, title=f'Hard Margin SVM (C=100) {SRN}')

"""

 This code makes up some simple data that can mostly be split by a line, but with a couple of points that are a bit out of place (outliers).
 This cell trains two different linear SVMs on that data – one that's strict about the line (hard margin) and one that's more flexible and allows some mistakes (soft margin)."""

SRN = "PES2UG24CS824" # Your SRN

# Soft Margin SVM (small C)
# TODO: Create a linear SVM model with a small C value (e.g., 0.1) for a soft margin.
#          - Set the kernel to 'linear'.
#          - Set C to 0.1.
#          - Set random_state to 42 for consistent results.
svm_soft = SVC(kernel='linear', C=0.1, random_state=42)

# TODO: Fit the soft margin model to the training data (X_train_linear_scaled, y_train_linear).
svm_soft.fit(X_train_linear_scaled, y_train_linear)

# TODO: Replace with your SRN
plot_decision_boundaries(X_train_linear_scaled, y_train_linear, svm_soft, title=f'Soft Margin SVM (C=0.1) {SRN}')

# Hard Margin SVM (large C)
# TODO: Create a linear SVM model with a large C value (e.g., 100) for a hard margin.
#          - Set the kernel to 'linear'.
#          - Set C to 100.
#          - Set random_state to 42.
svm_hard = SVC(kernel='linear', C=100, random_state=42)

# TODO: Fit the hard margin model to the training data
svm_hard.fit(X_train_linear_scaled, y_train_linear)

# TODO: Replace with your SRN
plot_decision_boundaries(X_train_linear_scaled, y_train_linear, svm_hard, title=f'Hard Margin SVM (C=100) {SRN}')